\section{Recommendations}
Our takeaway from this competition is that high quality data, reasonable models, and a little luck are the components for a successful modeling competition.  This is echoed by Gregory Matthews and Michael Lopez, the winning team.
\begin{quote}
Q: Were you surprised by any of your insights?

Greg: I was surprised by how well our simple models performed. Using the right data was MUCH more important to our models performing well than using more sophisticated models.

Mike: I think we gave ourselves a chance with a good model, but there was probably a decent amount of luck involved, too. Also, identifying the specific loss function for this Kaggle contest, and where it comes from, seemed to help our model.
\end{quote}

There seemed to be little advantage in using complex models.  In agreement with statements on Andrew Gelman's blog we found using score differential to be superior to win/loss:
\begin{quote}
Don’t model the probability of win, model the expected score differential. Yeah, I know, I know, what you really want to know is who wins. But the most efficient way to get there is to model the score differential and then map that back to win probabilities. The exact same issue comes up in election modeling: it makes sense to predict vote differential and then map that to Pr(win), rather than predicting Pr(win) directly. This is most obvious in very close games (or elections) or blowouts; in either of these settings the win/loss outcome provides essentially zero information. But it’s true more generally that there’s a lot of information in the score (or vote) differential that’s thrown away if you just look at win/loss.
\end{quote}
\andyc{citations for these blog/web interviews \& need to cleanly integrate these arguments with our thoughts.}

\subsection{Optimal Strategies}
\andyc{Marcos's piece here}
-subsection in recommendations: How should a typical user use this to figure out their bracket?
 (Marcos)
