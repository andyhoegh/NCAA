\section{Evaluation}
The evaluation section contains three distinct components: an analysis of the performance of a few popular methods, an overview of the Kaggle predictions, and a demonstration of the Nearest Neighbor Matchup Effects.
\subsection{Popular Methods}
March Madness prediction draws the attention of sport analysts and medias: notably, Nate Silver, Ken Pomeroy and the ESPN. In this section, we will compare these three models against Kaggle entries and examine the ``luck'' factor of Kaggle competition.

Because Silver and Pomeroy published probabilities of each team advancing, our first comparison focused on first round games (round of 64).  We noticed that all three models concurred in win-loss predictions except for one game (Gonzaga vs OKST, 8 vs 9 in West). All three models have a similar pattern based on the predictions' deviation from 0.5: while most predictions either suggest a easy win (prediction close to 1) or a close game (prediction close to 0.5), predictions around 0.75 are relatively rare in all three models.

In addition, Silver's prediction adopted a more aggressive style than the other two. His prediction, on average, has the highest deviation from 50-50 prediction and Pomeroy has the lowest. But this difference seems to be irrelevant to their scores. For the first around games, Pomeroy has the best score at 0.4632, followed by Silver 0.4664 and ESPN 0.4709. These scores ranked 38th, 45th and 64th respectively, among 433 Kaggle entries. 

We also computed a full-scale, team-to-team prediction based on the marginal advancement probabilities and found both the score and ranking of three models have dropped markedly from first round: ESPN (0.5795, 88th), Silver(0.5988, 123rd) and Pomeroy (0.6278, 174th). 

Another interesting finding in this comparison is the ``luck'' factor. What if a close game finished differently? To that end, we flipped the five overtime games in the first round and noticed a completely reverse of the ranking of popular models: ESPN(0.4631, 11th), Silver(0.5025,117th) and Pomeroy (0.5027,120th). This phenomenon is observed in other Kaggle entries as well. A 0.7169 Kendall's Tau is measured between the rankings before and after flipping. 

To sum up, popular models exhibit similarities in many aspects,  but none has shown clear advantages over most Kaggle entries. 

\subsection{Kaggle Leaderboard}
\andyc{is this section appropriate here?  Consider the flow of the paper once  the pieces come together}
\subsection{Nearest Neighbor Matchup Effects}
To demonstrate the efficacy of our method, we first fit Equation~\ref{eq:RS} using a well known rating system, the Sagarin ratings. After fitting this model, $\rho$ is calibrated based on historical results from the previous seven years NCAA tournaments. Seven years was chosen because this is the length of the complete history of the team level characteristics used to find neighbors of teams. The log loss for 2014 for the entire range of $\rho$ can be seen in Figure~\ref{fig:result} and in particular the reduced loss for the $\rho=0.2$ that was selected based on historical calibration.
\begin{figure}[h!]
\centering
\includegraphics[width=.7\textwidth]{results_2014.pdf}
\caption{Solid Line: Log loss for $\rho$, dashed vertical line: $\rho=.2$ value selected by historical calibration}
\label{fig:result}
\end{figure} 
A modest improvement is also seen in classification error from (0.365 to 0.350) although this is only a single game difference. The matchup effect, particularly with smaller $\rho$ values, will have a lesser effect on classification error than that of the loss functions like the log loss. This is because it will only shift the expected point differential a small margin, so the only games in which classification error would change are those that are nearly dead heat games to begin with.

To illustrate the matchup effects, consider Table~\ref{tab:change} which contains the ten games that saw the largest shift in expected point differential. This table contains expected point differentials (team1 - team2) denoted as Point Diff for the relative strength model (RS) as well as the matchup effects (ME).  Similarly probabilities of team 1 winning and realized loss for each model are depicted.  Finally the actual point differential is shown.
\begin{table}[h!]
\caption{ Ten games with largest point differential change when implementing Nearest Neighbor Matchup Effects model versus a relative strength model}
\scriptsize
\centering
\begin{tabular}{|cc | ccc | ccc | c|c|}
  \hline
  \hline
 team 1 & team 2 & Point Diff:RS & Prob:RS & Loss:RS & Point Diff:ME & Prob:ME & Loss:ME & Point Diff\\ 
  \hline
 Cal Poly & Wichita St & -18.69 & 0.04 & 0.04 & -17.10 & 0.06 & 0.06 &  -27\\ 
 UConn & St. Joes &4.29 & 0.65 & 0.43 & 6.18 & 0.71 & 0.34 & 8\\ 
 Dayton & Stanford & -2.16 & 0.42 & 0.86 & 0.94 & 0.53 & 0.63 & 10 \\ 
 Dayton & Syracuse & -6.34 & 0.28 & 1.27 & -4.05 & 0.36 & 1.03 & 2\\ 
 Kentucky & Michigan & -3.71 & 0.37 & 1.00 & -2.08 & 0.42 & 0.86 &  3\\ 
 UMass & Tennessee &-3.05 & 0.39 & 0.49 & -4.83 & 0.33 & 0.40 & -19\\ 
 Memphis & Virginia & -6.34 & 0.28 & 0.33 & -8.91 & 0.21 & 0.23 & -18\\ 
 Michigan & Tennessee & 5.37 & 0.69 & 0.37 & 3.49 & 0.62 & 0.47 & 2\\ 
 Michigan & Texas & 8.05 & 0.77 & 0.26 & 5.85 & 0.70 & 0.35 & 14\\ 
 Syracuse & W. Mich. & 12.65 & 0.88 & 0.13 & 15.01 & 0.92 & 0.09 & 24\\ 
   \hline
   \hline
\end{tabular}
\label{tab:change}
\end{table}
The key takeaway from this table is the loss for the predictions under each methodology. On this particular subset of games, the ME model performs considerably better than the typical model under the log loss (.446 to .520), the other games see minimal matchup effects, therefore, the results are similar.  To further illustrate how the nearest neighbor match effects functions consider the Dayton versus Stanford game.  Coincidentally this is the only game which the actual predicted winner changes, while many saw sizable shifts in predictive probabilities which factor into the loss function used in the Kaggle competition.  For each team, Table \ref{tab:DayStan} displays the neighbors, that is opponents that they faced most similar to the current matchup, along with the expected differential, realized result, and residual in those games. 
\begin{table}[h!]
\caption{Dayton - Stanford Neigbors \& Residuals}
\small
\centering
\begin{tabular}{|c|cccc |}
   \hline
   \hline
 team & neighbor &  Point Diff& Exp. Point Diff & Residual \\
  \hline
Dayton & California & 18 & -0.9 & 18.9\\
Dayton & Gonzaga & 5 & -12.4 & 17.4\\
Dayton & George Mason& 17 & 3.4 & 13.6\\
Dayton &  Georgia Tech& 10 & -5.3 & 15.3\\
Dayton & George Washington& 10 & 0.4 & 9.6\\
\hline
Stanford & California&-7 & 4.1&-11.1 \\
Stanford & California &11 &-2.3 &13.3 \\
Stanford & Oregon&2 &-8.9 &10.9 \\
Stanford & Pittsburgh&-21 &-5.3 &-15.7 \\
Stanford & Cal Poly&17 &13.8 &3.2 \\
Stanford & Utah&1 &4.9 &-3.9 \\
   \hline
   \hline
\end{tabular}
\label{tab:DayStan}
\end{table}
To avoid confusion one of Stanford's five neighbors was the University of California at Berkeley, who they faced twice.  The difference in expected point differential is driven largely by the fact that the first game was at Stanford while the second was at Berkeley. From this table we see that Dayton performed exceptionally well against teams our model considered to be similar to Stanford, in fact they were about 15 points better on average. The line shifted more than 3 points for this game because $\rho$ was calibrated to 0.2.
